{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858b1945-89ba-4446-b518-b86023a20ac3",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">**Capítulo 1**</span>\n",
    "## <font color=blue size=8> Procesos de Ciencia de Datos </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79653133-abea-46fb-87e6-b44b320ecaae",
   "metadata": {},
   "source": [
    "Con ofertas de trabajo lucrativas y millones en ciencia de datos, hay muchos que aspiran a ser científicos de datos. Se asume que necesitas tener un título universitario en estadística, informática o ingeniería. Sin embargo, hoy incluso los no técnicos buscan un trabajo como científico de datos. Con la ruta de aprendizaje adecuada, es posible para la mayoría de ustedes convertirse en científicos de datos, sin importar su educación universitaria. Este capítulo comienza con el proceso de ciencia de datos y establece el camino para su objetivo de convertirse en científico de datos. Si ya ocupas un puesto de científico de datos, el libro te proporcionará pautas para convertirte en un científico de datos “moderno”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8970346-943c-4622-be68-9a5643fdaf25",
   "metadata": {},
   "source": [
    "El primer paso para convertirse en científico de datos es comprender el proceso de la ciencia de datos. ¿Cuál es, en resumen, el papel de un científico de datos moderno? ¿Cómo ha desarrollado las aplicaciones de inteligencia artificial con un éxito sobresaliente que usamos en nuestra vida cotidiana? Usamos etiquetado facial, Alexa y autos autónomos: estas son aplicaciones de inteligencia artificial. En cuanto a las empresas, ¿cómo utilizan la inteligencia artificial? Gigantes tecnológicos como Amazon construyen sus propios sistemas de recomendación para la venta de productos. Los bancos usan la inteligencia artificial para la detección de fraudes y alertas tempranas. Incluso los gobiernos emplean la inteligencia artificial para la vigilancia, el monitoreo del tráfico, y así sucesivamente. Las aplicaciones son muchas. Los científicos de datos modernos desarrollaron todas esas aplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a229c4-beb3-4de7-8c83-178b1f593cfb",
   "metadata": {},
   "source": [
    "He estado usando el adjetivo “moderno” por una razón. Hoy, con el cambio tecnológico y las nuevas demandas de la industria, el papel de un científico de datos ha cambiado notablemente. Tradicionalmente, los científicos de datos utilizaban soluciones estadísticas bien estudiadas para desarrollar modelos predictivos de aprendizaje automático. Estos modelos se desarrollaban principalmente con datos numéricos disponibles en nuestras bases de datos tradicionales, ya sean relacionales o no relacionales. Incluso si los datos contenían algunas columnas de caracteres, las convertíamos en numéricas, ya que la máquina solo entiende el formato binario. La cantidad de estas columnas y el número de puntos de datos en los primeros días del aprendizaje automático eran lo suficientemente bajos para que las máquinas de esa época pudieran manejarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39655926-6562-4a08-9d6f-a5b75ef10a4e",
   "metadata": {},
   "source": [
    "Hoy en día, los requerimientos de la industria se basan en desarrollar modelos de aprendizaje automático (ML) sobre un enorme corpus de datos de texto e imágenes. Desarrollar estos modelos requiere recursos computacionales enormes y un procesamiento de datos previo completamente diferente en comparación con los datos numéricos extraídos de bases de datos tradicionales. Además, existen muchos avances tecnológicos en la construcción misma de los modelos. Para convertirse en un científico de datos moderno, es necesario entender cómo manejar estos nuevos tipos de datos y aprender las tecnologías modernas del aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079073ba-81d6-434a-b1f4-c4af286ab34a",
   "metadata": {},
   "source": [
    "Por lo tanto, comencemos nuestro viaje entendiendo primero el proceso de la ciencia de datos. Primero te presentaré el proceso tradicional de construcción de modelos, seguido por los científicos de datos de la vieja escuela. No, no lo tomes a mal. Aunque estos procesos se desarrollaron hace muchos años, todavía tienen utilidad en la ciencia de datos moderna. Te proporcionaré pautas claras sobre cuándo usar el enfoque tradicional y cuándo emplear un enfoque moderno y más avanzado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb3783-5626-43f0-8c86-1e6284b88046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Construcción de Modelo Tradicional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660da11f-0fc1-43c1-8371-1e6c0f3a2566",
   "metadata": {},
   "source": [
    "Durante todos estos años, un científico de datos, al construir una aplicación de inteligencia artificial, comenzaba primero con el análisis de datos exploratorio (por sus siglas en inglés EDA). Después de todo, entender los datos por uno mismo es fundamental para explicarle a la máquina qué significan. En términos técnicos, es importante para nosotros comprender las características (variables independientes) en nuestro conjunto de datos para realizar un análisis predictivo sobre la variable objetivo, que es la variable dependiente. Usando estas características y objetivos, crearíamos un conjunto de datos de entrenamiento para entrenar un algoritmo estadístico. Este EDA, muchas veces, requiere un conocimiento profundo del dominio. Y es precisamente ahí donde las personas con conocimiento en diversas industrias verticales prosperan para convertirse en científicos de datos. Intentaré cumplir con las aspiraciones de cada uno de esos individuos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccc3af-8156-477d-8155-c5d0ee579726",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, en aquellos días los datos eran mayormente numéricos. Como científico de datos, debes asegurarte de que los datos estén limpios antes de alimentarlos a un algoritmo. Por eso entra en juego la limpieza de datos. Para esto, primero hay que identificar si existen valores faltantes. En caso afirmativo, se debe eliminar esas columnas del análisis o imputar los valores adecuados en esos campos vacíos. Una vez asegurada la limpieza de los datos, es necesario realizar un preprocesamiento para prepararlos para el aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56288f-660f-455b-aa42-fbefeb7678d5",
   "metadata": {},
   "source": [
    "Los distintos pasos requeridos en la preparación de datos incluyen estudiar la varianza de los datos en las columnas, escalar los datos, buscar correlaciones, reducir la dimensionalidad, entre otros. Para ello, se utilizan múltiples herramientas disponibles para explorar datos, obtener representaciones visuales de distribuciones y correlaciones entre columnas, así como aplicar diversas técnicas de reducción de dimensionalidad. La lista es extensa; el proceso es laborioso y consume mucho tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534a71a-adb9-40d7-bab3-dd60eedb940e",
   "metadata": {},
   "source": [
    "Después de que el científico de datos prepara el conjunto de datos para el aprendizaje automático, su siguiente tarea es seleccionar un algoritmo apropiado basado en su conocimiento y experiencia. Una vez que el algoritmo está entrenado, decimos que hemos construido el modelo. El científico de datos entonces utiliza métodos conocidos de evaluación de desempeño para probar el modelo entrenado con sus conjuntos de datos de prueba. Si las métricas de desempeño no arrojan precisiones aceptables, intentará ajustar los hiperparámetros del algoritmo. Si esto no funciona, puede que tenga que regresar a la etapa de preparación de datos, seleccionar nuevas características, realizar ingeniería adicional de características y aplicar más reducciones de dimensionalidad, para luego volver a entrenar su algoritmo y mejorar la precisión. Si esto tampoco funciona, probará con otro algoritmo estadístico. Todo este proceso continúa a través de muchas iteraciones hasta que logra crear un modelo afinado para su conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597ad12-ef69-4d49-bd6c-e3e51dbf58d8",
   "metadata": {},
   "source": [
    "Como ves, el proceso no solo consume mucho tiempo, sino que también requiere un conocimiento sólido en métodos estadísticos. Es fundamental entender varios algoritmos clásicos de aprendizaje automático y poseer un conocimiento profundo de técnicas de análisis exploratorio de datos (EDA). Dicho de otra manera, se requiere un conocimiento amplio más que profundo de cada algoritmo para desarrollar modelos de aprendizaje automático eficientes.\n",
    "Ahora, veamos el enfoque moderno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea27f3-d002-459f-817e-490d78a5c8a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Enfoque Moderno de la Construcción de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534965c4-805c-4c52-af31-2cd034e8a906",
   "metadata": {},
   "source": [
    "El proceso tradicional que he descrito anteriormente es sumamente laborioso y consume mucho tiempo. No solo eso, como científico de datos necesitas tener un conocimiento excelente en estadística como materia, manejar varias herramientas disponibles para el análisis de datos exploratorio (EDA), aprender diversos algoritmos de aprendizaje automático y conocer las distintas técnicas para evaluar el desempeño del modelo. ¿Qué pasaría si alguien automatizara todo este proceso? Esa automatización ha estado disponible durante muchos años. Como científico de datos, necesitas adquirir estas nuevas habilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67ffcc-38e3-41de-9bee-f1e49b1b36e0",
   "metadata": {},
   "source": [
    "Contamos con auto-sklearn, que funciona sobre la popular biblioteca sklearn de aprendizaje automático, y que realiza tanto la construcción automática de modelos para regresión como para clasificación. No solo selecciona el modelo, sino que incorpora funcionalidades integradas para construir una tubería de datos (data pipeline) con mejor rendimiento mediante la aplicación de optimización bayesiana. También utiliza técnicas estadísticas de ensamblaje para realizar agregaciones de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12bd88e-4fb7-4362-800e-6378346c1e25",
   "metadata": {},
   "source": [
    "Si decides usar un enfoque con una red neuronal artificial (en inglés ANN) para construir tu modelo, tenemos AutoKeras, que genera el diseño de red con mejor desempeño para tu conjunto de datos. Existen múltiples herramientas comerciales y no comerciales en este ámbito; solo por mencionar algunas, están H2O.ai, TPOT, MLBox, PyCaret, DataRobot, DataBricks y BlobCity AutoAI. Esta última ofrece una característica interesante que, hasta donde sé y en el momento de escribir esto, nadie más ofrece: genera un archivo .ipynb completo (el código fuente del proyecto), que como científico de datos puedes reclamar como tu propia invención. Después de todo, todos los clientes exigen originalidad y el código fuente del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a4abb-b9fa-426b-abb5-a3b2ed3ec9f8",
   "metadata": {},
   "source": [
    "He incluido un capítulo completo sobre AutoML para que puedas dominar este enfoque novedoso.\n",
    "El siguiente aspecto importante del desarrollo moderno en aprendizaje automático es la introducción de nuevos tipos de datos, y esos son los datos de texto e imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22989ba6-4027-42d5-99e3-1205b3706880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inteligencia Artificial en Conjuntos de Datos de Imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82ad2f-a446-45b2-addf-5a57e1584119",
   "metadata": {},
   "source": [
    "Con el éxito sobresaliente en el desarrollo de modelos predictivos utilizando técnicas estadísticas, a las que también llamamos aprendizaje automático clásico (classical ML) y, en ocasiones, incluso usamos el término “inteligencia artificial a la antigua” (good old-fashioned AI, GOFAI), la industria quiso aplicar sus aprendizajes inicialmente a datos de imágenes para la detección de objetos. Probablemente comenzó con la detección de una cierta clase de objetos en una imagen, y luego las técnicas se extendieron para clasificar perros y gatos, etiquetar rostros, identificar personas y finalmente incluso para realizar estas operaciones en flujos de datos en tiempo real. Hoy en día, tenemos aplicaciones de aprendizaje automático como el análisis de tráfico que funcionan con flujos de datos en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b6741-3f9f-4394-9cf2-eaa26c4bb20f",
   "metadata": {},
   "source": [
    "Todos estos nuevos modelos hicieron que los ingenieros de aprendizaje automático (a corto plazo, los llamamos ingenieros ML) y científicos de datos desarrollaran y aprendieran nuevos métodos para procesar datos de imágenes. Finalmente, nuestras computadoras solo entienden datos binarios; aunque los datos de imagen son datos binarios, aún necesitamos transformar estos datos en un formato comprensible para la máquina. Cabe destacar que cada imagen está compuesta por numerosos elementos de datos binarios (píxeles) y que cada píxel en una imagen es una representación RGB dentro de un archivo de datos de imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef59ed-f888-489a-96da-c7243ec0f2ec",
   "metadata": {},
   "source": [
    "La tecnología clásica y antigua del aprendizaje automático no pudo satisfacer estos nuevos requerimientos, por lo que la industria empezó a buscar enfoques alternativos. La tecnología de redes neuronales artificiales (ANNs), que fue inventada hace varias décadas, acudió al rescate. Los recursos computacionales modernos hicieron viable el uso de esta tecnología para desarrollar dichos modelos. El entrenamiento de redes neuronales requiere varios gigabytes de memoria, GPUs y muchas horas de entrenamiento. Los gigantes tecnológicos contaban con esos recursos, entrenaron redes que podemos reutilizar y extender sus funcionalidades para nuestros propios propósitos. A esta nueva tecnología la llamamos aprendizaje por transferencia (Transfer Learning). He incluido una cobertura exhaustiva sobre esta nueva tecnología más adelante en el libro — aprendizaje por transferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe898d7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Desarrollo de Modelos en Conjuntos de Datos de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4237ec",
   "metadata": {},
   "source": [
    "Después de observar el éxito de la tecnología ANN/DNN (redes neuronales artificiales/redes neuronales profundas) en la creación de aplicaciones para imágenes, los investigadores comenzaron a explorar su aplicación en datos de texto. Así surgió un nuevo término y un campo propio: el procesamiento de lenguaje natural (en inglés NLP). Preparar datos de texto para el aprendizaje automático requiere un enfoque diferente al que se usa para datos numéricos. Los datos numéricos se almacenan en bases de datos con pocas columnas, y cada columna es un posible candidato para una característica. Por ello, en los modelos de aprendizaje automático con datos numéricos, el número de características suele ser bajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d335a3-74fa-4451-be16-d5475cd1cbc7",
   "metadata": {},
   "source": [
    "Ahora, considera los datos de texto, donde cada palabra o frase puede ser una característica. En aplicaciones para detectar correo no deseado (spam), se utiliza cada palabra del texto como característica, mientras que para un modelo de resumen de documentos, cada oración será una característica. Si consideramos el vocabulario completo de palabras, es fácil imaginar la enorme cantidad de características. Las técnicas tradicionales de reducción de dimensionalidad aplicadas a datos numéricos no son útiles para reducir las dimensiones en conjuntos de datos de texto. Es necesario limpiar todo el corpus de texto. Esta limpieza implica varios pasos, como eliminar puntuación, palabras vacías (stop words), eliminar o convertir números, pasar todo a minúsculas, entre otros. Además de la limpieza, para reducir la cantidad de características, se aplican técnicas como construir un diccionario de palabras únicas, stemming, lematización, y más. Finalmente, es fundamental entender la tokenización, para transformar esas palabras u oraciones en formatos comprensibles para la máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feea793-d7ba-4033-ba65-b99632926258",
   "metadata": {},
   "source": [
    "En los datos de texto, el contexto en el que aparece una palabra es muy importante para su análisis. Por ello, surgió una nueva rama llamada comprensión del lenguaje natural (en inglés NLU). Se introdujeron conceptos como bolsa de palabras (bag-of-words, BoW), tf-idf (frecuencia de término inversa de documento), bi-gramas, n-gramas, entre otros. A lo largo de los años, los investigadores desarrollaron muchas arquitecturas nuevas de redes neuronales para crear aplicaciones de inteligencia artificial basadas en texto, comenzando con las RNN (redes neuronales recurrentes), continuando con LSTM (Long-Short Term Mmeory) (memoria a largo y corto plazo), y actualmente con los transformadores y sus diferentes implementaciones. Hoy en día, existen muchos modelos basados en la arquitectura de transformadores. BERT (representaciones codificadas bidireccionalmente de transformadores) es uno de los más utilizados. También están modelos como GPT para generación de texto. La lista es interminable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df784bad",
   "metadata": {},
   "source": [
    "### Construcción de Modelos con Conjuntos de Datos de Alta Frecuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3b4f3",
   "metadata": {},
   "source": [
    "Los conjuntos de datos de alta frecuencia son otro desafío importante que ha surgido en la industria actualmente. ¿Por qué digo que es un reto? Porque la frecuencia del conjunto de datos cambia constantemente con el tiempo: los datos son dinámicos. Por lo tanto, todo el proceso de ciencia de datos, que incluye la construcción de tuberías de datos eficientes, la ingeniería de características y el entrenamiento y evaluación del modelo, debe ser dinámico también. Empresas como Amazon y Jio (una gran compañía de telecomunicaciones en India) recopilan varios terabytes de datos diariamente. Procesar y analizar datos con un volumen tan alto y frecuencia tan elevada no es una tarea sencilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b31d9-9228-4dbc-9d8d-b5111ae17ebd",
   "metadata": {},
   "source": [
    "Habiendo entendido los requerimientos y necesidades para desarrollar aplicaciones modernas de inteligencia artificial, ahora te mostraré el flujo de trabajo que sigue un científico de datos moderno para crear esos modelos de aprendizaje automático tan reconocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307794c-aa6d-4353-a25b-9fd3b65edd29",
   "metadata": {},
   "source": [
    "## Proceso de Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d842e2f-fb24-4beb-a6c3-c815b10d331c",
   "metadata": {},
   "source": [
    "El primer paso en la construcción del modelo es preparar los datos para el entrenamiento del algoritmo o de la red neuronal artificial (ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1594f-b942-4332-bc16-4a9ad4a2d9f4",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed13f0-a274-4db8-85ac-dcae9ff0db3b",
   "metadata": {},
   "source": [
    "Los datos en el mundo se presentan en dos formatos: numérico o basado en caracteres. Una tabla de base de datos puede contener tanto campos numéricos como campos de caracteres. Los datos basados en caracteres también pueden encontrarse en documentos de texto. Cuando se realiza análisis de datos utilizando aprendizaje automático, los datos pueden estar en una base de datos relacional, ser estructurados o no estructurados, o incluso provenir de una fuente NoSQL. Además, un científico de datos debe trabajar con grandes corpus de texto, como noticias o novelas. Si obtenemos datos de la web, estos contendrán etiquetas HTML. También se requiere que el científico de datos trabaje con conjuntos de datos de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90028c",
   "metadata": {},
   "source": [
    "Tanto los datos numéricos como los basados en texto requieren tratamientos de procesamiento diferentes. Comencemos por considerar los datos numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dd0be-4997-4638-b39a-f56cf313f4c1",
   "metadata": {},
   "source": [
    "### Procesamiento de Datos Numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf0e57-6203-4dea-8b6f-5831db841c3e",
   "metadata": {},
   "source": [
    "El uso de datos numéricos se aplica en desarrollos como la predicción del clima, la estimación de ventas durante Navidad, la decisión del precio de oferta para una casa en una subasta, entre otros.\n",
    "\n",
    "El flujo de trabajo necesario para procesar datos numéricos se muestra en la figura 1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a657a22-08cb-486d-a7e2-ce384eb2f93c",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.1.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.1</p>\n",
    "\n",
    "<p>Procesamiento de campos numéricos.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8803a-f2a6-4f93-886f-6416499a6a33",
   "metadata": {},
   "source": [
    "Como sabes, muchas veces la base de datos puede contener valores nulos en un campo numérico. Si cuentas con una cantidad considerable de datos para aprendizaje automático, podrías decidir eliminar las filas que contienen esos campos nulos. Sin embargo, si no tienes suficientes puntos de datos, lo más común es reemplazar esos valores nulos con la media o la mediana de la columna.\n",
    "Si la base de datos no está indexada, es posible encontrar filas duplicadas en una tabla; estas deben eliminarse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd925ab",
   "metadata": {},
   "source": [
    "Cada columna en la base de datos puede tener una distribución diferente, y entre columnas el rango mínimo-máximo puede variar. Por ello, todos los datos por columna deben normalizarse y escalarse a una misma escala. Generalmente, los datos se escalan a un rango de -1 a +1 o de 0 a 1, para facilitar un mejor entrenamiento del algoritmo.\n",
    "Ahora, hablaré sobre qué tipo de procesamiento se requiere para los datos basados en texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8b10-af63-4f5c-8397-cc6e4364c71f",
   "metadata": {},
   "source": [
    "### Procesamiento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a59ac3-d448-4559-8dd7-52e464b30aa9",
   "metadata": {},
   "source": [
    "El preprocesamiento de los datos de texto depende del tipo de aplicación que estés tratando de desarrollar. Un campo de caracteres en una columna de base de datos puede contener valores como “Masculino” y “Femenino” para indicar el género. Si este campo es importante para tu modelo, debes reemplazar “Masculino” por 0 y “Femenino” por 1, por ejemplo. Es decir, debemos convertir estos datos categóricos en valores numéricos, ya que las computadoras solo entienden datos binarios y no comprenden caracteres. A esta conversión la llamamos codificación (encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a683dd-f071-40ab-b311-c0420d0af492",
   "metadata": {},
   "source": [
    "Existen dos tipos de codificación que usamos: LabelEncoder y One-Hot Encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825e83c-2b64-45bc-bf3a-f7257bad609c",
   "metadata": {},
   "source": [
    "Ahora bien, hay otro tipo de desarrollo de modelos de aprendizaje automático que requiere un tratamiento diferente para datos basados en caracteres. Considera una aplicación para detectar correo no deseado (spam). Si un correo contiene ciertas palabras como “lotería” o “ganador”, podría ser clasificado como spam. Por tanto, una aplicación de cliente de correo debe clasificar los mensajes que contienen estas palabras en la categoría de spam, mientras que los demás serán considerados como no spam. Para esto, es necesario tokenizar todo el texto del correo en palabras para detectar la presencia de dichas palabras. Consideramos cada palabra del texto como una característica para el aprendizaje automático. Solo imagina la cantidad de palabras y, por ende, el número de características que un documento puede aportar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68457365",
   "metadata": {},
   "source": [
    "Normalmente, en los conjuntos de datos disponibles en bases de datos no encontramos este tipo de situación, ya que cada columna de la base de datos es considerada una característica para el aprendizaje automático, y el número de columnas suele ser bajo y fácilmente manejable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffff29",
   "metadata": {},
   "source": [
    "Por lo tanto, el procesamiento de datos de texto para aplicaciones como detección de spam, resumen de texto y clasificación de películas requiere muchos más pasos que el procesamiento de datos numéricos. A este proceso lo llamamos procesamiento de lenguaje natural (NLP). En algunas aplicaciones avanzadas, como la traducción de idiomas, es necesario aplicar técnicas de comprensión del lenguaje natural (NLU). Hoy en día también vemos aplicaciones como la generación de noticias falsas, que requieren un vocabulario amplio y muchas características con las que trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2828cb",
   "metadata": {},
   "source": [
    "### Preprocesamiento de Datos de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eda038",
   "metadata": {},
   "source": [
    "Un flujo de trabajo general para procesar datos de texto se muestra en la Fig. 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569918bf",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.2.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.2</p>\n",
    "\n",
    "<p>Procesamiento del corpus de texto.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d231f0",
   "metadata": {},
   "source": [
    "Primero, eliminamos todos los signos de puntuación del texto. ¿Por qué? Porque incluso los signos de puntuación serían tokenizados y agregados a nuestra lista de características. Luego, el texto contiene varias palabras vacías, como “el,” “esto,” “en,” y otras similares. Estas palabras tienen poco significado cuando realizamos categorización de documentos o análisis de sentimiento, por lo que las eliminamos del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd469fb",
   "metadata": {},
   "source": [
    "Después de este procesamiento básico, avanzamos un paso más hacia nuestro siguiente objetivo: la extracción de características. Para esto, tokenizamos el documento para extraer palabras u oraciones. Por ejemplo, tokenizaríamos el texto en oraciones cuando queramos resumir un pasaje. Ahora describiré brevemente la tokenización de palabras. Cuando tokenizas el texto en palabras, puedes encontrar la ocurrencia de palabras como “run,” “running,” y “ran”; estas palabras transmiten un solo significado, que es correr, representado como una sola palabra clave “run” que se añade a nuestro repositorio de características. A este proceso lo llamamos stemming. Aquí solo te di un ejemplo, pero para procesar texto sin procesar se necesitan muchas más técnicas como lematización, etiquetado gramatical (POS tagging), vectorización, chunking, chinking, entre otras. El procesamiento de lenguaje natural (NLP) es un ámbito completamente diferente, y describiré este preprocesamiento por separado en el Capítulo 18 (Aplicaciones basadas en ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a5e5b",
   "metadata": {},
   "source": [
    "Nuestro siguiente flujo de trabajo es comprender los datos que acabas de limpiar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4961854",
   "metadata": {},
   "source": [
    "### Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82dae6",
   "metadata": {},
   "source": [
    "La Figura 1.3 muestra el flujo de trabajo para el análisis exploratorio de datos (EDA), cuyo objetivo es comprender mejor los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf31c9-3a66-4579-a543-e2e25752a905",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.3.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.3</p>\n",
    "\n",
    "<p>Librerías para visualización de datos.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b1b42",
   "metadata": {},
   "source": [
    "Para comprender los datos, existen varias librerías disponibles para la visualización de datos. Las más comúnmente utilizadas en Python son Matplotlib y Seaborn, las cuales pueden ofrecer un nivel muy avanzado de visualización. Puedes crear diagramas de dispersión, histogramas, gráficos de barras, gráficos circulares, gráficos tridimensionales, contornos, gráficos polares, y cualquier otro tipo que necesites para entender mejor los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e9f20",
   "metadata": {},
   "source": [
    "La Figura 1.4 muestra algunas capacidades de Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231a99-cbd6-4c8e-b6d7-01cc291382b1",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.4.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.4</p>\n",
    "\n",
    "<p>Ejemplos de outputs con Matplotlib.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158367c9",
   "metadata": {},
   "source": [
    "La Figura 1.5 presenta algunos gráficos avanzados creados con Seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfbbe18-0d88-4cf2-81dc-c6eb3970fa97",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.5.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.5</p>\n",
    "\n",
    "<p>Ejemplos de outputs con Seaborn.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45eaba2",
   "metadata": {},
   "source": [
    "Estas visualizaciones de datos son de gran ayuda para detectar valores atípicos, entender correlaciones, distribuciones de datos, entre otros aspectos. Utilizamos esta información para refinar aún más nuestros conjuntos de datos de entrenamiento.\n",
    "A continuación, pasamos a la ingeniería de características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89653911",
   "metadata": {},
   "source": [
    "Ingeniería de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff0691",
   "metadata": {},
   "source": [
    "Seleccionar características adecuadas manteniendo el número de características bajo es fundamental para entrenar el modelo de manera eficiente. A este proceso lo llamamos ingeniería de características. El flujo de trabajo de la ingeniería de características tiene dos caminos: selección de características y reducción de dimensionalidad, como se muestra en la Figura 1.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b24b2-25c5-499a-ac05-50c60f622f27",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.6.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.6</p>\n",
    "\n",
    "<p>Opciones en ingeniería de características.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea2fe5",
   "metadata": {},
   "source": [
    "La selección de características consiste en elegir y excluir ciertas características sin modificarlas. La reducción de dimensionalidad disminuye la cantidad de características reduciendo sus dimensiones mediante diversas técnicas desarrolladas por investigadores a lo largo de los años."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5332b",
   "metadata": {},
   "source": [
    "Con conocimiento del dominio, puedes eliminar manualmente las características no deseadas mediante inspección directa y simplemente eliminando esas columnas de la base de datos. También puedes remover características que contienen valores faltantes, presentan baja varianza o están altamente correlacionadas. Entre las técnicas que puedes usar se encuentran la selección univariante de características o la eliminación recursiva de características. Incluso puedes utilizar la función SelectFromModel de sklearn para eliminar características cuyos valores estén por debajo de un umbral establecido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25514649",
   "metadata": {},
   "source": [
    "Para reducir las características, también puedes aplicar técnicas de reducción de dimensionalidad. Una técnica ampliamente utilizada es el Análisis de Componentes Principales (PCA). Esta técnica es especialmente útil en casos de multicolinealidad excesiva o cuando la explicación de los predictores no es una prioridad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ced6a7",
   "metadata": {},
   "source": [
    "La ingeniería de características es un tema complejo. No te preocupes por ello ahora, ya que lo explicaré con mayor detalle en un capítulo posterior dedicado a la reducción de dimensionalidad.\n",
    "Una vez que hayas terminado con la selección de características y del objetivo, estarás listo para construir el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b6c1a",
   "metadata": {},
   "source": [
    "### Decisión sobre el Tipo de Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47bf75",
   "metadata": {},
   "source": [
    "Clasificamos un problema de aprendizaje automático en tres categorías principales, como se muestra en la Figura 1.7. No incluyo aquí el clustering, ya que requiere un tratamiento especial; he dedicado una sección completa, que abarca varios capítulos, a ese tema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60012c6c",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.7.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.7</p>\n",
    "\n",
    "<p>Selección del modelo según la tarea.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fb309",
   "metadata": {},
   "source": [
    "Cuando deseas predecir un valor específico, llamado objetivo en términos de aprendizaje automático, desarrollas un modelo de regresión. Cuando quieres agrupar un conjunto de datos en un número conocido o desconocido de grupos, utilizas clasificación. Un ejemplo de esto es un modelo de segmentación de clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a666f2",
   "metadata": {},
   "source": [
    "Si estás desarrollando un modelo de inteligencia artificial para jugar ajedrez, o cualquier otro juego que requiera aprendizaje continuo, desarrollarás un modelo de aprendizaje por refuerzo. Como no cubro el desarrollo de juegos ni otras aplicaciones que usan aprendizaje por refuerzo en este libro, no profundizaré en ese tipo de modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b951651",
   "metadata": {},
   "source": [
    "Ahora, enfoquémonos en los modelos de regresión y clasificación, que son donde reside la mayoría de las necesidades de la industria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27786073",
   "metadata": {},
   "source": [
    "### Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32453726",
   "metadata": {},
   "source": [
    "Para ambos tipos de modelos, regresión y clasificación, el entrenamiento puede realizarse mediante aprendizaje supervisado o no supervisado, como se observa en la Figura 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b43bcf",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.8.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.8</p>\n",
    "\n",
    "<p>Aprendizaje supervisado/no supervisado.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a680d38",
   "metadata": {},
   "source": [
    "En el caso del aprendizaje supervisado, es necesario contar con un conjunto de datos etiquetado, es decir, para cada punto de datos en el conjunto, se conoce el valor objetivo. Usando este conjunto etiquetado, el modelo ajustará sus hiperparámetros para prepararse a inferir sobre datos desconocidos. La precisión del modelo puede evaluarse usando un conjunto de validación, que es una parte del conjunto etiquetado original, pero que no se utiliza durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd047f",
   "metadata": {},
   "source": [
    "En el aprendizaje no supervisado, no se cuenta con datos etiquetados, o bien, es imposible crear un conjunto de datos etiquetado debido a su tamaño. En estos casos, se emplean algoritmos de aprendizaje automático que analizan el conjunto de datos por sí mismos. Por ejemplo, un modelo de detección de objetos como OpenCV o YOLO fue entrenado utilizando aprendizaje no supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be541d",
   "metadata": {},
   "source": [
    "### Selección del Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e792f",
   "metadata": {},
   "source": [
    "Una de las tareas más desafiantes para un científico de datos es decidir qué algoritmo utilizar. Tanto para problemas de regresión como de clasificación, existen numerosos algoritmos disponibles en nuestros repositorios. El reto consiste en seleccionar el más adecuado para el conjunto de datos y que pueda lograr una alta precisión al predecir datos no vistos previamente. Este libro te ayudará a entender varios algoritmos y a elegir el apropiado para tu aplicación. Para darte una visión rápida de los algoritmos disponibles para aprendizaje automático, observa la Figura 1.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31505ac3",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.9.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.9</p>\n",
    "\n",
    "<p>Lista exhaustiva de modelos clásicos de aprendizaje automático.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5275a",
   "metadata": {},
   "source": [
    "Es fundamental que un científico de datos comprenda estos algoritmos, si no en su implementación completa, al menos en sus conceptos fundamentales. Si entiendes conceptualmente el algoritmo y su propósito, sabrás cuál usar para la necesidad actual. Después de todo, hay expertos que han implementado estos algoritmos de forma eficiente, y probablemente no haremos un mejor trabajo que esos desarrolladores expertos. Aunque la implementación no sea totalmente optimizada, en la mayoría de los casos sigue siendo aceptable. Solo cuando tu modelo realiza inferencias en tiempo real es que se justifica una optimización de alto nivel. Créeme, casi todas las librerías disponibles en el mercado cuentan con código optimizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37a49e",
   "metadata": {},
   "source": [
    "Incluso para las redes neuronales profundas (DNN), existe un vasto repositorio de arquitecturas y algoritmos, resumidos en la Figura 1.10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd51abc",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.10.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.10</p>\n",
    "\n",
    "<p>Lista exhaustiva de modelos/algoritmos de redes neuronales.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f47930",
   "metadata": {},
   "source": [
    "Ahora surge la pregunta: si utilizo otro algoritmo de aprendizaje automático para resolver el mismo problema, ¿obtendré mejor precisión? La respuesta no es trivial, y muchos se frustran probando manualmente diferentes algoritmos en el mismo conjunto de datos hasta encontrar el que funciona mejor. Afortunadamente, la tecnología ha avanzado y existe ayuda disponible para seleccionar el algoritmo adecuado. Esto se llama AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2bd50",
   "metadata": {},
   "source": [
    "## AutoML (Auto Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7915cf5",
   "metadata": {},
   "source": [
    "La Figura 1.11 representa el proceso de AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f603f3",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.11.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.11</p>\n",
    "\n",
    "<p>Proceso de AutoML.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc540b3",
   "metadata": {},
   "source": [
    "Existen múltiples librerías y frameworks de AutoML disponibles en el mercado, tanto gratuitos como comerciales. Una de las librerías de código abierto más populares es auto-sklearn. Estas librerías y frameworks ofrecen preparación automática de datos, selección del modelo de aprendizaje automático que mejor desempeño tiene sobre el conjunto de datos, y ajuste de los hiperparámetros del algoritmo seleccionado. En resumen, cubren todo el proceso de aprendizaje automático, que hasta ahora ni siquiera he descrito completamente. Todo lo que necesitas hacer es proporcionarles un conjunto de datos, y ellos te sugerirán el modelo mejor ajustado y listo para producción. Además, presentan un ranking de modelos basado en sus puntajes de precisión, y tú decides cuál usar según tus necesidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c3b7a",
   "metadata": {},
   "source": [
    "Actualmente, compañías como Google, Microsoft y Amazon ofrecen aprendizaje automático como servicio (MLaaS), donde simplemente subes tu conjunto de datos a sus servidores y, finalmente, descargas una tubería de modelo que puede ser alojada y usada vía un servicio web. Aunque esto es fascinante, un verdadero científico de datos puede superar estas soluciones automatizadas. Además, la mayoría de estos servicios siguen un enfoque de caja negra y no proporcionan el código fuente del desarrollo del modelo para que puedas modificarlo. Un científico de datos suele usar estos servicios automatizados para reducir rápidamente las opciones en la selección del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfb048",
   "metadata": {},
   "source": [
    "Para tus propios propósitos y desarrollo, puedes usar librerías gratuitas como auto-sklearn, AutoKeras, o versiones comerciales como H2O.ai. He incluido una lista exhaustiva de estos frameworks en el capítulo de AutoML. Estas librerías soportan múltiples modelos de aprendizaje automático para entrenamiento y evaluación. Entrenan cada modelo con tu conjunto de datos, evalúan su rendimiento y finalmente clasifican los modelos según sus puntajes de precisión. ¿No es genial? Tal vez te cueste creerlo, pero espera a que te muestre una implementación real en un capítulo posterior. No solo seleccionan el algoritmo, también afinan los parámetros y te entregan un modelo listo para usar. Algunas librerías de AutoML, como BlobCity, incluso proporcionan el código fuente completo para la construcción del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560207b",
   "metadata": {},
   "source": [
    "## Ajuste de Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117eb5b3",
   "metadata": {},
   "source": [
    "El siguiente flujo de trabajo, como se observa en la Figura 1.12, consiste en afinar los hiperparámetros del algoritmo seleccionado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc06a48",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.12.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.12</p>\n",
    "\n",
    "<p>Proceso de ajuste de hiperparámetros.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a91585",
   "metadata": {},
   "source": [
    "Después de decidir qué algoritmo utilizar, el siguiente paso es entrenarlo con un conjunto de datos de entrenamiento. Cada algoritmo utiliza varios parámetros que necesitan ser ajustados cuidadosamente. A este proceso lo llamamos ajuste de hiperparámetros, que es iterativo. Podemos verificar el ajuste evaluando la precisión del modelo en un conjunto de datos de validación. Para la optimización de hiperparámetros, puedes utilizar frameworks como Optuna. Una vez alcanzado el nivel deseado de precisión, el modelo estará listo para su uso en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d5380",
   "metadata": {},
   "source": [
    "Hasta ahora, los flujos de trabajo que he mostrado te ayudan a crear un modelo de aprendizaje automático basado en modelado estadístico tradicional. También llamamos a esto GOFAI (Good Old-Fashioned Artificial Intelligence), o inteligencia artificial clásica. A continuación, definiré un flujo de trabajo para desarrollar modelos basados en redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe03f0",
   "metadata": {},
   "source": [
    "## Construcción de Modelos Usando Redes Neuronales Artificiales (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caddc031",
   "metadata": {},
   "source": [
    "Como sabes, el diseño de las redes neuronales artificiales (ANN) se inspira en la estructura del cerebro humano. Al igual que nuestro cerebro, una ANN está compuesta por varias capas, y cada capa contiene múltiples nodos, que equivalen a las neuronas en nuestro cerebro. Una arquitectura típica de ANN se muestra en la Figura 1.13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29aac7",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.13.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.13</p>\n",
    "\n",
    "<p>Esquema de ANN/DNN.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cf119",
   "metadata": {},
   "source": [
    "Tu tarea al diseñar una arquitectura de ANN consiste en decidir cuántas capas tendrá, qué tipo de conexiones existirá entre ellas y cuántos nodos habrá en cada capa. El flujo completo para el desarrollo de una ANN se presenta en la Figura 1.14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8e23a",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.14.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.14</p>\n",
    "\n",
    "<p>Proceso de desarrollo de ANN.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08030e",
   "metadata": {},
   "source": [
    "\n",
    "Una vez que defines la arquitectura, introduces los datos en la red, permitiendo que estos se propaguen a través de ella para producir resultados. Basándose en esos resultados, la red realiza un proceso llamado retropropagación para ajustar los hiperparámetros, es decir, los pesos asignados a cada conexión. Este proceso es iterativo, y tras varias rondas de entrenamiento, conocidas como épocas en términos de aprendizaje automático, la red te ofrecerá un nivel aceptable de precisión. Si la precisión no es suficiente, puedes intentar alimentar a la red con más datos para que entienda mejor la información. Si esto tampoco mejora los resultados, tendrás que regresar al punto de partida y modificar el número de capas, la cantidad de nodos en cada capa, los algoritmos de optimización, las funciones de error establecidas para el aprendizaje de la red, entre otros aspectos. Describiré este proceso en detalle cuando trate el desarrollo de modelos basados en ANN en capítulos posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7b8c5",
   "metadata": {},
   "source": [
    "Aunque puedes diseñar tu propia arquitectura ANN para aplicaciones pequeñas y conjuntos de datos medianos, crear una arquitectura grande no es sencillo. Estas arquitecturas requieren muchos recursos y tiempos de entrenamiento que pueden extenderse por semanas. Modelos de reconocimiento de imágenes y detección de objetos, como R-CNN y YOLO, se desarrollaron sobre arquitecturas complejas que demandan recursos enormes y largos períodos de entrenamiento. A estas redes las llamamos redes neuronales profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c3042",
   "metadata": {},
   "source": [
    "¿Podemos usar modelos preentrenados de estas redes para nuestros propios fines? La respuesta a esta pregunta es un rotundo sí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d13a77",
   "metadata": {},
   "source": [
    "## Modelos Basados en Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5b05d",
   "metadata": {},
   "source": [
    "Para entender el término transfer learning, permíteme darte un ejemplo. Contamos con modelos preentrenados de clasificación de imágenes que pueden detectar un perro en una foto. Pero, ¿qué pasa si quiero saber la raza del perro? Estos modelos, que detectan cientos de objetos diferentes, rara vez se adentran en clasificaciones tan específicas. Ahora podemos extender esos modelos preentrenados agregando nuestras propias capas de red y entrenando únicamente esas capas adicionales para determinar la raza del perro. Esto es justamente lo que llamamos transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91058c0f",
   "metadata": {},
   "source": [
    "El flujo de trabajo para el desarrollo de modelos con transfer learning se muestra en la Figura 1.15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60920e1",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"fig1.15.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "<p style=\"color:blue; margin-top: 5px; font-weight: bold;\">Fig. 1.15</p>\n",
    "\n",
    "<p>Proceso de aprendizaje por transferencia.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c75b0",
   "metadata": {},
   "source": [
    "En nuestro ejemplo, utilizamos el modelo preentrenado para reconocimiento de objetos. Dichos modelos han sido entrenados con millones de imágenes, requiriendo muchas semanas de entrenamiento en GPUs o TPUs. Contar con ese poder de procesamiento y ese tiempo para entrenar está fuera del alcance de la mayoría. Solo los gigantes tecnológicos disponen de infraestructuras capaces de realizar entrenamientos tan extensos. Afortunadamente, ellos nos permiten usar sus modelos preentrenados. Podemos extender esos modelos, agregar algunas capas adicionales y entrenarlas para nuestro propósito específico, como predecir la raza del perro después de que el modelo detecta un perro en la imagen. Más adelante en el libro te mostraré un ejemplo concreto de cómo se realiza este proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5437b53",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed3f7e",
   "metadata": {},
   "source": [
    "Te he presentado el proceso completo de la ciencia de datos, tal como lo sigue un científico de datos avanzado. Para construir modelos de aprendizaje automático eficientes, necesitas desarrollar varias habilidades. Por lo general, la profundidad de cada habilidad es superficial. Para ser un científico de datos exitoso, lo que realmente se requiere es un conocimiento amplio, no profundo. Finalmente, un científico de datos no es un investigador encargado de desarrollar nuevos algoritmos o técnicas de visualización. Debes enfocarte en entender los conceptos que hay detrás de estas tecnologías y cómo usarlas para construir tus propios modelos. Lo que importa es decidir cuál utilizar y en qué contexto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
